# -*- coding: utf-8 -*-
"""ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jCTqcydFt2p5x3_43ekKfpFp1I78abzD
"""

# CSV and code from https://github.com/krishnaik06/Complete-Deep-Learning/tree/master/ANN

!pip install scikeras
!pip install keras_tuner

"""#**Importing the libraries**"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""#**Importing the dataset**"""

dataset = pd.read_csv('Churn_Modelling.csv')

# To include columns 3-->12 as the input
X = dataset.iloc[:, 3:13]

# To include column 13 as the output
y = dataset.iloc[:, 13]
dataset

"""#**Encoding**"""

# Encoding geography and gender columns (One-hot encoding)
geography = pd.get_dummies(X["Geography"], drop_first=True)
gender = pd.get_dummies(X['Gender'], drop_first=True)

X = pd.concat([X,geography,gender],axis=1)

# Drop Unnecessary columns
X = X.drop(['Geography','Gender'],axis=1)
X

"""#**Splitting the dataset into the Training set and Test set**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
X_train

"""#**Feature scaling**"""

# Feature scaling is a EDA mechanism which scales down all the input parameters to a value lower enough which won't disturb the training data. e.g., 1000 is scaled down to 1.052, 100000 scaled down to 1.24...
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
X_train[0]

"""#**Importing the Keras libraries and packages**"""

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout

"""#**Real Job starts here**"""

# Initialising the ANN
classifier = Sequential()


# 1st hidden layer ==> 6 neurons
# weights to be initilized using 'he_uniform' technique
# activation function ==> ReLU/Leaky ReLU
# 11 input columns from our dataset
classifier.add(Dense(units = 6,
                     kernel_initializer = 'he_uniform',
                     activation ='relu',
                     input_dim = 11))

# 2nd hidden layer ==> 6 neurons
# weights to be initilized using 'he_uniform' technique
# activation function ==> ReLU/Leaky ReLU
classifier.add(Dense(units = 6,
                     kernel_initializer = 'he_uniform',
                     activation ='relu'))

# output layer ==> 1 neuron
# weights to be initilized using 'glorot_uniform' technique
# activation function ==> sigmoid
classifier.add(Dense(units = 1,
                     kernel_initializer = 'glorot_uniform',
                     activation = 'sigmoid'))

# Loss ==> 'binary_crossentropy' since the o/p is either 0 or 1
# Optimizer (for loss reduction) ==> Adam/Adamax
classifier.compile(optimizer = 'Adamax',
                   loss = 'binary_crossentropy',
                   metrics = ['accuracy'])

# Fitting the ANN to the Training set
# Note: Validation data and test data is different
# No. of epochs ==> 100
# batch size ==> 10, such that '10' rows of data is loaded into the RAM everytime and our system is not burdened enough
model_history = classifier.fit(X_train,
                               y_train,
                               validation_split=0.33,
                               batch_size = 10,
                               epochs = 100)

# Ouput legends:
# 536/536 ==> no. of batches each of size mentioned in the object above
# accuracy ==> accuracy of the train dataset
# loss ==> loss from train dataset
# val_accuracy ==> accuracy of the validation dataset
# val_loss ==> loss from validation dataset

"""# **Making the predictions and evaluating the model**"""

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Anything beyond 0.5 is considered to be a valid o/p
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Calculate the Accuracy
from sklearn.metrics import accuracy_score
score = accuracy_score(y_pred,y_test)
score

"""#**Deciding Number of Hidden Layers And Neuron In Neural Network - Keras Tuner**"""

from keras import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras_tuner import RandomSearch

def build_model(hyperparameter):
  # Initialising the ANN
  model = Sequential()

  # To test on '2-20' hidden layers
  for i in range(hyperparameter.Int('num_layers', 2, 20)):

    # Every hidden layer to try out b/w '32-512' neurons
    model.add(
      Dense(
        units = hyperparameter.Int(
            'units_' + str(i),
            min_value = 32,
            max_value = 512,
            step = 32),
        kernel_initializer = 'he_uniform',
        activation='relu'))

  # Final/Output layer
  model.add(Dense(1, kernel_initializer = 'glorot_uniform', activation='sigmoid'))

  # Loss ==> 'binary_crossentropy' since the o/p is either 0 or 1
  # Optimizer (for loss reduction) ==> Adam/Adamax
  model.compile(
    optimizer = Adam(hyperparameter.Choice('learning_rate', [0.001, 0.0001, 0.00001])),
    loss = 'binary_crossentropy',
    metrics = ['accuracy'])
  return model

tuner = RandomSearch(
  build_model,
  objective = 'val_accuracy',
  max_trials = 5,
  executions_per_trial = 3,
  directory = 'project_2',
  project_name = 'Air Quality Index')
tuner.search_space_summary()

tuner.search(
  X_train,
  y_train,
  epochs = 5,
  batch_size = 10,
  validation_data = (X_test, y_test))

tuner.results_summary()

"""#**Deciding Number of Hidden Layers And Neuron In Neural Network - scikit learn GridSearchCV (Cross-validation)**"""

from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV
from keras.layers import Activation

def create_model(layers, activation):
  # Initialising the ANN
  model = Sequential()

  for i, nodes in enumerate(layers):

    # For the first layer add the input dimension
    if i==0:
        model.add(Dense(nodes, input_dim = X_train.shape[1]))
        model.add(Activation(activation))

    # Anything hidden layer after 1st layer should not have input dimension
    else:
        model.add(Dense(nodes))
        model.add(Activation(activation))

  # Creating the final layer
  model.add(Dense(1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

  # Compiling the model
  model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

  return model

# This model will be used further for all other tasks like doing cross-validation and stuff
model = KerasClassifier(model = create_model, verbose = 0)

layers = [[20], [40, 20], [45, 30, 15]]
activations = ['sigmoid', 'relu']

# create_model() has two parameters ==> 'layers, activation', so, to refer to these properly we should use ==> "'model__layers': layers, 'model__activation': activations" as shown below
params = {'model__layers': layers, 'model__activation': activations, 'batch_size': [128, 256], 'epochs': [30]}

# This will check all the parameters provided and bring out the best results amongst them
grid = GridSearchCV(estimator = model, param_grid = params, cv = 5)

grid_result = grid.fit(X_train, y_train)

[grid_result.best_score_,grid_result.best_params_]

pred_y = grid.predict(X_test)
y_pred = (pred_y > 0.5)

from sklearn.metrics import accuracy_score
score = accuracy_score(y_test, y_pred)
score

